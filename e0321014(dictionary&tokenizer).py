# -*- coding: utf-8 -*-
"""E0321014(dictionary&tokenizer.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tNekZk84zBmpUX-JUCVXlS2PxNOPxKKv
"""

import nltk

paragraph = "Hello i am ram from chennai, i am 18 years old."

sentences = paragraph.split(".")

tokenized_sentences = [sentence.strip().split() for sentence in sentences if sentence.strip()]

for sentence in tokenized_sentences:
    print(sentence)

vocabulary = set([word for sentence in tokenized_sentences for word in sentence])

print("\nVocabulary:", vocabulary)

sentences = [
    "I don't like bananas",
    "They're always late",
    "This was co-created by my friend",
    "This document was signed \"12-02-24\""
]

tokenized_sentences = [sentence.split() for sentence in sentences]

vocabulary = set([word for sentence in tokenized_sentences for word in sentence])

print("Tokenized Sentences:")
for sentence in tokenized_sentences:
    print(sentence)

print("\nVocabulary:", vocabulary)

def create_dictionary(documents):

    dictionary = {}

    for doc in documents:

        words = doc.split()

        for word in words:

            word = word.strip(',.!?:;\"\'()[]{}<>\t\n')

            word = word.lower()

            if word not in dictionary:
                dictionary[word] = 1
            else:
                dictionary[word] += 1

    return dictionary

documents = [
    """ scikit-learn is largely written in Python, and uses NumPy extensively for high-performance linear algebra and array operations. Furthermore, some core algorithms are written in Cython to improve performance. Support vector machines are implemented by a Cython wrapper around LIBSVM; logistic regression and linear support vector machines by a similar wrapper around LIBLINEAR. In such cases, extending these methods with Python may not be possible.
scikit-learn integrates well with many other Python libraries, such as Matplotlib and plotly for plotting, NumPy for array vectorization, Pandas dataframes, SciPy, and many more.
We present iNLTK, an open-source NLP library consisting of pre-trained language models and out-of-the-box support for Data Augmentation, Textual Similarity, Sentence Embeddings, Word Embeddings, Tokenization and Text Generation in 13 Indic Languages. By using pre-trained models from iNLTK for text classification on publicly available datasets, we significantly outperform previously reported results. On these datasets, we also show that by using pre-trained models and data augmentation from iNLTK, we can achieve more than 95% of the previous best performance by using less than 10% of the training data. iNLTK is already being widely used by the community and has 40,000+ downloads, 600+ stars and 100+ forks on GitHub. The library is available at https://github.com/goru001/inltk.&quot;
The Natural Language Toolkit, or more commonly NLTK, is a suite of libraries and programs for symbolic and statistical natural language processing (NLP) for English written in the Python programming language. It supports classification, tokenization, stemming, tagging, parsing, and semantic reasoning functionalities.[4] It was developed by Steven Bird and Edward Loper in the Department of Computer and Information Science at the University of Pennsylvania.[5] NLTK includes graphical demonstrations and sample data. It is accompanied by a book that explains the underlying concepts behind the language processing tasks supported by the toolkit,[6] plus a cookbook. """
]

dictionary = create_dictionary(documents)

print("Dictionary:")
print(dictionary)

